¡Bien! Como vemos `StandardScaler` transforma el conjunto de datos para sus variables se aproximen a una distribución normal, de forma que todos datos tenga una varianza del mismo orden. De este modo, cada dato nos dará una idea de a cuántos desvíos de la media está ese punto.

Este tipo de procedimientos son muy importantes al realizar cualquier tipo de análisis de datos, y no sólo al trabajar con distancias euclídeas. Imaginemos que tenemos que analizar la trayectoria profesional de dos personas, para hacer una selección laboral. A priori, sería lógico pensar en basar esta selección en el curriculum de dichas personas. Sin embargo, resulta evidente que el curriculum no nos da un panaroma completo de las habilidades de una persona. Por ejemplo, no nos permite conocer su capacidad de trabajo en equipo o sus habilidades para realizar más de una tarea a la vez, etc. ¿Qué peso le estamos dando entonces a estas otras características? ¿Estamos subvalorando o sobrevalorando las hablidades de las personas? ¿Que pasa con las personas no tienen las mismas posibilidades para completar su curriculum? ¿Las hace menos capaces para el trabajo? 

En otras palabras, como ilustra la [esta historia breve](https://cajondeherramientas.com.ar/index.php/2016/05/05/en-bandeja-de-plata-una-historia-sobre-los-privilegios/) de [Toby Morris](https://en.wikipedia.org/wiki/Toby_Morris_(cartoonist)) es que resulta necesario escalar los datos para que todas las características tengan la misma importancia y ninguna esté dominada por otra. 
